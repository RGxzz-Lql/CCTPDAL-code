import numpy as np
import torch
import torch.nn as nn
import utils
import torch.nn.functional as F
from sklearn import metrics
import time
from pytorch_pretrained.optimization import  BertAdam
from importlib import import_module
import pandas as pd
import re
import matplotlib.pyplot as plt
import os
acc = []
lossdev = []
batch = []
def train(config, model, train_iter, dev_iter, test_iter,name,dataset):
    """
    模型训练方法
    :param config:
    :param model:
    :param train_iter:
    :param dev_iter:
    :param test_iter:
    :return:
    """
    name = name
    dataset = dataset
    #print(name)
    start_time = time.time()
    #启动 BatchNormalization 和 dropout
    model.train()
    #拿到所有mode种的参数
    param_optimizer = list(model.named_parameters())
    # 不需要衰减的参数
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']

    optimizer_grouped_parameters = [
        {'params':[p for n,p in param_optimizer  if not any( nd in n for nd in no_decay)], 'weight_decay':0.01},
        {'params':[p for n,p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_deacy':0.0}
    ]

    optimizer = BertAdam(params = optimizer_grouped_parameters,
                         lr=config.learning_rate,
                         warmup=0.05,
                         t_total=len(train_iter) * config.num_epochs)

    total_batch = 0 #记录进行多少batch
    dev_best_loss = float('inf') #记录校验集合最好的loss
    last_imporve = 0 #记录上次校验集loss下降的batch数
    flag = False #记录是否很久没有效果提升，停止训练
    model.train()
    for epoch in range(config.num_epochs):
        print('Epoch [{}/{}'.format(epoch+1, config.num_epochs))
        with open("./%s/%s/%straindev.txt" % (dataset, name, name), 'a')as f:
            f.write('Epoch [{}/{}\n'.format(epoch+1, config.num_epochs))
        #print(train_iter)
        for i, (trains, labels,content) in enumerate(train_iter):
            outputs = model(trains)
            model.zero_grad()
            loss = F.cross_entropy(outputs, labels)
            loss.backward(retain_graph=False)
            optimizer.step()#4不训练过程前向反向  传递
            if total_batch % 1 == 0: #每多少次输出在训练集和校验集上的效果
                true = labels.data.to(config.device).cpu().numpy()
                predit = torch.max(outputs.data, 1)[1].cpu().numpy()
                #print(type(true))
                #print(type(predit))
                train_acc = metrics.accuracy_score(true, predit)
                dev_acc, dev_loss = evaluate(config, model, dev_iter)
                dev_acc= dev_acc.item()
                dev_loss = dev_loss
                acc.append(dev_acc)
                lossdev.append(dev_loss)
                #loss.append(dev_loss)
                total_batch = total_batch
                #loss=dev_loss
                batch.append(total_batch)
                if dev_loss < dev_best_loss:
                    dev_best_loss = dev_loss.cpu()
                    torch.save(model.state_dict(), config.save_path)
                    imporve = '*'
                    last_imporve = total_batch
                else:
                    imporve = ''
                time_dif = utils.get_time_dif(start_time)
                msg = 'Iter:{0:>6}, Train Loss:{1:>5.2}, Train Acc:{2:>6.2}, Dev Loss:{3:>5.2}, Dev Acc:{4:>6.2%}, Time:{5} {6}'
                print(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, imporve))
                with open("./%s/%s/%straindev.txt"%(dataset,name,name),'a')as f:
                    f.write(msg.format(total_batch, loss.item(), train_acc, dev_loss, dev_acc, time_dif, imporve+'\n'))

                model.train()
            total_batch = total_batch + 1
            # if total_batch - last_imporve > config.require_improvement:
            #     #在验证集合上loss超过1000batch没有下降，结束训练
            #     print('在校验数据集合上已经很长时间没有提升了，模型自动停止训练')
            #     flag = True
            #     break

        # if flag:
        #     break
    draw_curve(batch, lossdev, acc, dataset, name)
    test(config, model, test_iter,name,dataset)

def evaluate(config, model, dev_iter, test=False):
    """
    :param config:
    :param model:
    :param dev_iter:
    :return:
    """
    model.eval()
    loss_total = 0
    predict_all = np.array([], dtype=int)
    labels_all = np.array([], dtype=int)
    with torch.no_grad():
        for (texts, labels, content) in dev_iter:
            outputs = model(texts)
            loss = F.cross_entropy(outputs, labels)
            loss_total = loss_total + loss
            labels = labels.data.cpu().numpy()
            predict = torch.max(outputs.data,1)[1].cpu().numpy()
            labels_all = np.append(labels_all, labels)
            predict_all = np.append(predict_all, predict)

    acc = metrics.accuracy_score(labels_all, predict_all)
    if test:
        report = metrics.classification_report(labels_all, predict_all, target_names=config.class_list, digits=4)
        confusion = metrics.confusion_matrix(labels_all, predict_all)
        return acc, loss_total / len(dev_iter), report, confusion

    return acc, loss_total / len(dev_iter)


def test(config, model, test_iter,name,dataset):
    """
    模型测试
    :param config:
    :param model:
    :param test_iter:
    :return:
    """
    model.load_state_dict(torch.load(config.save_path))
    model.eval()
    start_time = time.time()
    test_acc, test_loss ,test_report, test_confusion = evaluate(config, model, test_iter, test = True)
    msg = 'Test Loss:{0:>5.2}, Test Acc:{1:>6.2%}'
    print(msg.format(test_loss, test_acc))
    print("Precision, Recall and F1-Score")
    print(test_report)
    print("Confusion Maxtrix")
    print(test_confusion)
    time_dif = utils.get_time_dif(start_time)
    print("使用时间：",time_dif)
    with open("./%s/%s/%stest.txt" % (dataset, name, name), 'a')as f:
        #f.write(msg.format(test_loss, test_acc+'\n'))
        f.write('Test Loss:%s Test Acc:%s '%(test_loss, test_acc)+'\n')
        f.write("Precision, Recall and F1-Score"+'\n')
        f.write(test_report)
        #f.write(test_confusion)


def train_test(config,model,test_iter):
    df_empty = pd.DataFrame()
    score = [[]]

    with torch.no_grad():
        for texts, labels,content in test_iter:
            #list_i[i for in rang(n_batches)] = []
            lists = []

            outputs = model(texts)
            #outputs = outputs.cpu().numpy()
            predit = torch.max(outputs.data, 1)[1].cpu().numpy()

            #content = content[0]
            #y = torch.LongTensor([item[1] for item in datas]).to(self.device)  # 标签数据label
            labels = labels.cpu().numpy().tolist()
            #labels = labels.cpu().numpy().item()
            #predit_label = torch.max(outputs.data, 1)[1].item()
            predit_label = torch.max(outputs.data, 1)[1].cpu().numpy().tolist()
            #predit_confidence = torch.max(outputs.data, 1)[0].item()
            predit_confidence = torch.max(outputs.data, 1)[0].cpu().numpy().tolist()

            df_empty[0].append(content)
            df_empty[1].append(labels)
            df_empty[2].append(predit_label)
            df_empty[3].append(predit_confidence)
            # lists.append(content)
            # lists.append(labels)
            # lists.append(predit_label)
            # lists.append(predit_confidence)
            #
            # print(lists)
            # score.append(lists)
            print(df_empty)
    #del score[0]
    return  df_empty

def train_sort(list):
    list = sorted(list, key=(lambda x: x[3]))
    return list

# def train_segmentation(list1):
#     #print(list)
#     #del list[3]
#     list1 = list(map(lambda x: x[0:2], list1))
#     #print(list1)
#     return list1

def train_write(list):
    with open('unlabedleastactive11.txt','w',encoding="UTF-8")as f:
        for i in range(0,len(list)):
            f.writelines(str(list[i][0])+"\t"+str(list[i][2])+"\r")


def draw_curve(batch,loss,acc,dataset,name):
    fig = plt.figure(figsize=(32, 16))
    ax0 = fig.add_subplot(121, title="loss")
    ax1 = fig.add_subplot(122, title="acc")
    ax0.plot(batch,loss, 'bo-', label='train')
    ax1.plot(batch, acc, 'bo-', label='train')
    # if current_epoch == 0:
    #     ax0.legend()
    #     ax1.legend()
    #fig.savefig("./%s/%s/%straindev.txt"%(dataset,name,name), name, 'train.svg')
    fig.savefig(os.path.join(".\\%s\\%s"%(dataset,name),  'dev.svg'))








